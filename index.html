<!doctype html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-21408087-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-21408087-2');
    </script>

    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>Alex Yu's Website</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="manifest" href="site.webmanifest">
    <link rel="apple-touch-icon" href="icon.png">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="css/style_v3.css">
</head>

<body>
    <div class="wrapper">
        <nav id="sidebar">
            <div id="sidebar-wrapper">
                <div class="sidebar-header">
                    <h1><span class="first-name">Alex</span> <span class="last-name">Yu</span></h1>
                    <p>Personal Website</p>
                </div>

                <ul class="list-unstyled components">
                     <li class="active" > <a href="index.html">About</a> </li> 
                </ul>
                <ul id="links" class="list-unstyled">
                    <li>
                        <a class="mail">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M20,8L12,13L4,8V6L12,11L20,6M20,4H4C2.89,4 2,4.89 2,6V18A2,2 0 0,0 4,20H20A2,2 0 0,0 22,18V6C22,4.89 21.1,4 20,4Z" />
                            </svg>
                            <script type="text/javascript">document.write('\u0073\u0078\u0079\u0075\u0040\u0062\u0065\u0072\u006b\u0065\u006c\u0065\u0079\u002e\u0065\u0064\u0075')</script>
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=LltfiXgAAAAJ">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M12,3L1,9L12,15L21,10.09V17H23V9M5,13.18V17.18L12,21L19,17.18V13.18L12,17L5,13.18Z" />
                            </svg>
                            Google Scholar
                        </a>
                    </li>
                    <li> <a href="https://github.com/sxyu">
                            <svg style="width:20px;height:20px" class="mr-1" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
                            </svg>
                            sxyu</a>
                    </li>
                </ul>
            </div>
        </nav>

        <div id="content">
            
    <div id="top-container">
        <div id="profile-image">
            <img src="img/photo.jpg" title="Photo of Alex" style="width:128px;height:128px;margin-right:2em;border-radius:50%">
        </div>
        <div class="about">
            <p>
            Hi! My name is Alex and I'm the co-founder and CTO at <a href="https://lumalabs.ai">Luma AI</a>, where I am working on generative models. We recently released
            <a href="https://lumalabs.ai/dream-machine">dream-machine</a>, a frontier video generative model, and previously I also co-led the development of
            <a href="https://lumalabs.ai/genie">genie</a>, a text-to-3d model.<br/>
            For those interested: Please see <a href="https://lumalabs.ai/join">this link</a> for open positions.
            </p>
            <p>
            Previously, I graduated from UC Berkeley, where I majored in CS and applied math and
            worked with <a href="https://people.eecs.berkeley.edu/~kanazawa/">Prof. Angjoo Kanazawa</a> in BAIR.
            While at Berkeley, I also interned at Google and Adobe, helped search for aliens at SETI: Breakthrough Listen, 
            particiated in programming competitions, and
            released various open source projects on <a href="https://github.com/sxyu">Github</a>, among other things.
            Before Berkeley, I lived in Vancouver, Canada for many years.
            </p>
        </div>
    </div>
    <div class="pt-4">
        <iframe id="luma-gallery"
            src=""
            style="height:450px;width:100%;padding-bottom:0.5em;padding-top:0.1em" frameborder="0" title="luma interactive scene, captured by me" style="border: none;"> </iframe>

    </div>
    <div class="pt-4">
        <h2>News</h2>
        <ul>
            <li> August 2024: Luma <a href="https://x.com/LumaLabsAI/status/1825639918539817101">released</a> Dream Machine 1.5
            <li> June 2024: Invited talk at CVPR 3D generation workshop <a href="https://ai3dg.github.io/">AI3DG</a> on Dream Machine and connecting it to our previous 3D work.
                 <a href="https://docs.google.com/presentation/d/1Izg0HYeH_H40Cpfk4MhcLF5ApAyxKsq109PuVyJFHSY/edit?usp=sharing">Slides are available here</a>.
            <li> June 2024: Luma <a href="https://x.com/LumaLabsAI/status/1800921380034379951">released</a> Dream Machine, our first video model
            <li> November 2023: Luma <a href="https://x.com/LumaLabsAI/status/1719792922646987152?lang=en">released</a> Genie, a 3D foundation model
            <li> August 2023: Co-organized and presented at <a href="https://neuralfields.cs.brown.edu/siggraph23.html">Neural Fields course</a> at SIGGRAPH 2023 
            <li> June 2023: Invited talk at CVPR <a href="https://sites.google.com/view/xrnerf/xrnerf-2023">XRNeRF workshop</a> with Amit Jain 
        </ul>
        <br/>
        <h2>Foundation models and research works</h2>
        <div id="pubs">
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <video muted autoplay loop><source src="img/dream-machine-landing.mp4" type="video/mp4"></video>
                    
                </div>
                <div class="col">
                    <h4>Dream Machine</h4>
                    <div class="pub-authors">
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        June 2024
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    
                    
                    <a href="https://lumalabs.ai/dream-machine">Website</a>
                    
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        A generative video model that generates 120 frames of high-fidelity video from a image or text prompt, in under 120 seconds. Built on a transformer architecture and trained directly on videos, dream machine shows a promising level of 3D and physical consistency. We followed this with advanced control capabilities such as keyframe-control and looping.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <video muted autoplay loop><source src="img/genie-landing.mp4" type="video/mp4"></video>
                    
                </div>
                <div class="col">
                    <h4>Genie: Text to 3D foundation model</h4>
                    <div class="pub-authors">
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        November 2023
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    
                    
                    <a href="https://lumalabs.ai/genie">Website</a>
                    
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        A foundation model for generating 3D assets from a text prompt within 10 seconds, along with a refine process to achieve higher quality models.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/plenoxels.gif" alt="Plenoxels: Radiance Fields Without Neural Networks">
                    
                </div>
                <div class="col">
                    <h4>Plenoxels: Radiance Fields Without Neural Networks</h4>
                    <div class="pub-authors">
                        
                        <a href="https://alexyu.net" class="link-self">Alex Yu*</a>, 
                        
                        <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil*</a>, 
                        
                        <a href="https://www.matthewtancik.com">Matt Tancik</a>, 
                        
                        <a href="https://www.linkedin.com/in/qinhong-chen/">Qinhong Chen</a>, 
                        
                        Benjamin Recht, 
                        
                        <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        CVPR 2022
                        <span class="pub-award"> Oral </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    <a href="https://arxiv.org/abs/2112.05131">arXiv</a>
                    
                    
                    
                    <a href="/plenoxels">Website</a>
                    
                    
                    <a href="https://www.youtube.com/watch?v=ElnuwpQEqTA&feature=emb_title">Video</a>
                    
                    
                    <a href="https://www.youtube.com/watch?v=yptwRRpPEBM">Two minute papers</a>
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/plenoctrees.jpg" alt="PlenOctrees for Real-time Rendering of Neural Radiance Fields">
                    
                </div>
                <div class="col">
                    <h4>PlenOctrees for Real-time Rendering of Neural Radiance Fields</h4>
                    <div class="pub-authors">
                        
                        <a href="https://alexyu.net" class="link-self">Alex Yu</a>, 
                        
                        <a href="https://www.liruilong.cn">Ruilong Li</a>, 
                        
                        <a href="https://www.matthewtancik.com">Matt Tancik</a>, 
                        
                        <a href="https://www.hao-li.com">Hao Li</a>, 
                        
                        <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html">Ren Ng</a>, 
                        
                        <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        ICCV 2021
                        <span class="pub-award"> Oral </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    <a href="https://arxiv.org/abs/2103.14024">arXiv</a>
                    
                    
                    
                    <a href="/plenoctrees">Website</a>
                    
                    
                    <a href="https://www.youtube.com/watch?v=obrmH1T5mfI">Video</a>
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS,  which is over 3000 times faster than conventional NeRFs.  We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects.  Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we  factorize the appearance via closed-form spherical basis functions.  Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network.  Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods.  Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully.  Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems.  PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/pixelnerf.jpg" alt="pixelNeRF: Neural Radiance Fields from One or Few Images">
                    
                </div>
                <div class="col">
                    <h4>pixelNeRF: Neural Radiance Fields from One or Few Images</h4>
                    <div class="pub-authors">
                        
                        <a href="https://alexyu.net" class="link-self">Alex Yu</a>, 
                        
                        <a href="https://people.eecs.berkeley.edu/~vye">Vickie Ye</a>, 
                        
                        <a href="https://www.matthewtancik.com">Matt Tancik</a>, 
                        
                        <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        CVPR 2021
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    <a href="http://arxiv.org/abs/2012.02190">arXiv</a>
                    
                    
                    
                    <a href="/pixelnerf">Website</a>
                    
                    
                    <a href="https://youtu.be/voebZx7f32g">Video</a>
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one).
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <div class="pub row pt-3 pb-3">
                <div class="col-sm-2 pub-preview">
                    
                    <img src="img/bl.png" alt="The Breakthrough Listen Search for Intelligent Life: Data Formats, Reduction and Archiving">
                    
                </div>
                <div class="col">
                    <h4>The Breakthrough Listen Search for Intelligent Life: Data Formats, Reduction and Archiving</h4>
                    <div class="pub-authors">
                        
                        Matthew Lebofsky et al.
                        
                    </div>
                    <div class="pub-venue pb-1">
                        <small>
                        Publications of the Astronomical Society of the Pacific (PASP)
                        <span class="pub-award">  </span>
                        </small>
                    </div>
                    <div class="pub-links">
                    
                    <a href="https://arxiv.org/abs/1906.07391">arXiv</a>
                    
                    
                    
                    
                    
                    </div>
                    <div class="pub-description pt-1">
                        
                        <p class="pub-abstract">
                        Breakthrough Listen is the most comprehensive and sensitive search for extraterrestrial intelligence (SETI) to date, employing a collection of international observational facilities including both radio and optical telescopes. During the first three years of the Listen program, thousands of targets have been observed with the Green Bank Telescope (GBT), Parkes Telescope and Automated Planet Finder. At GBT and Parkes, observations have been performed ranging from 700 MHz to 26 GHz, with raw data volumes averaging over 1PB / day... In this paper, we describe the hardware and software pipeline used for collection, reduction, archival, and public dissemination of Listen data.
                        </p>
                        
                    </div>
                </div>
            </div>
            
            <small>
            * = Equal contribution
            </small>
        </div>
    </div>
    <div class="timeline pt-2">
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/luma.png" alt="Luma AI">
            
            </div>
            <div class="timeline-time">2022-</div>
            <div class="timeline-org">
            Luma AI
            </div>
            <div class="timeline-position">
            Co-Founder
            </div>
            <div class="timeline-description">
            Series B AI start-up based in Palo Alto
            </div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/bair.png" alt="Berkeley AI Research">
            
            </div>
            <div class="timeline-time">Summer 2020-Fall 2021</div>
            <div class="timeline-org">
            Berkeley AI Research
            </div>
            <div class="timeline-position">
            Research Assistant
            </div>
            <div class="timeline-description">
            Advisor: <a href='https://angjookanazawa.com/'>Angjoo Kanazawa</a>; worked on research related to NeRFs (see below)
            </div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/adobe.png" alt="Adobe Research">
            
            </div>
            <div class="timeline-time">Summer 2021</div>
            <div class="timeline-org">
            Adobe Research
            </div>
            <div class="timeline-position">
            Research Intern
            </div>
            <div class="timeline-description">
            Host: <a href='https://research.adobe.com/person/oliver-wang/'>Oliver Wang</a>; worked on NeRF without COLMAP
            </div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/cs61a.png" alt="CS 61A @ UC Berkeley">
            
            </div>
            <div class="timeline-time">Fall 2019</div>
            <div class="timeline-org">
            CS 61A @ UC Berkeley
            </div>
            <div class="timeline-position">
            Teaching Assistant
            </div>
            <div class="timeline-description">
            Responsible for the traditional Hog Contest for several semesters before.
            </div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/google.png" alt="Google">
            
            </div>
            <div class="timeline-time">Summer 2019</div>
            <div class="timeline-org">
            Google
            </div>
            <div class="timeline-position">
            SWE Intern
            </div>
            <div class="timeline-description">
            Built banking and trading features for Google Assistant
            </div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/bsrc.png" alt="SETI: Breakthrough Listen">
            
            </div>
            <div class="timeline-time">Spring 2019</div>
            <div class="timeline-org">
            SETI: Breakthrough Listen
            </div>
            <div class="timeline-position">
            URAP Apprentice
            </div>
            <div class="timeline-description">
            Looking for alien technosignatures at SETI.
            </div>
        </div>
        
        <div class="timeline-entry">
            <div class="timeline-image-holder">
            
            <img src="img/timeline/fhlvive.png" alt="FHL Vive Center">
            
            </div>
            <div class="timeline-time">Fall 2017-Fall 2019</div>
            <div class="timeline-org">
            FHL Vive Center
            </div>
            <div class="timeline-position">
            Research Assistant
            </div>
            <div class="timeline-description">
            Worked on human and hand projects in OpenARK, with Dr. Allen Yang
            </div>
        </div>
        
    </div>
    <div class="coursework pt-2">
        <h2>Misc</h2>
        My Chinese name is 余思贤 / 余思賢.
    </div>
    <div class="pt-2">
        <a href="https://github.com/sxyu/sxyu.github.io"><small>Source code for this website</small></a>
    </div>
    <script>
        (function() {
            let lumaScenes = ['e24b0a79-1701-440f-b778-f8e6ac309b0e', '27d03b0e-2392-4ad9-adbf-9870969c65e9', 'af0ca347-8944-4d12-ac4f-21515a1fd04d', 'f1f0a102-8e5e-429d-ac09-181fa7a8edf4', '987de995-16a8-4322-b6df-71cb814bd148', '56b36e97-2fa6-4b4a-9e1d-3c7f1cecd990', '824c9005-7639-4b41-ab10-87a1d09e9745', '80c80dac-3154-4d5d-ad60-2372c4789c78', '6c1e44b7-baad-4af7-8551-b6f2f54f3873', '0b258a32-717d-49d7-8595-4bb481216396', '695eb78c-1a9e-4eff-8cd5-a9dcb391ec71', '04f59e29-4a67-4442-aa29-de5d4b7c7cd7', '7b40317e-462c-4498-bb62-d22b310576bf', '061cd3c1-21d1-4dd3-a15f-9d43acf2d88e', 'c9692be0-af0c-48c2-864f-2cbef8f1631f', 'f49a3b4a-8b9f-4394-b5c1-169d84cda4e9']
            let randomScene = lumaScenes[Math.floor(Math.random() * lumaScenes.length)];
            let galleryElem = document.getElementById('luma-gallery');
            galleryElem.src = 'https://lumalabs.ai/embed/' + randomScene + '?mode=sparkles&background=%23ffffff&color=%23000000&showTitle=true&loadBg=true&logoPosition=bottom-left&infoPosition=bottom-right&cinematicVideo=undefined&showMenu=false';
        })();
    </script

        </div>
    </div>
</body>


<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</html>