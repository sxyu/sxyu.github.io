<!DOCTYPE html>

<title>Plenoxels: Radiance Fields without Neural Networks</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-21408087-2"></script>
<script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
        dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-21408087-2');
</script>

<meta charset="utf-8">

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.4.0/css/bootstrap.min.css" crossorigin="anonymous">
<link rel="stylesheet" href="css/style.css">
<link rel="preconnect" href="https://fonts.gstatic.com">

<style>
    /* greek-ext */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 300;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fCBc4EsA.woff2) format('woff2');
        unicode-range: U+1F00-1FFF;
    }

    /* greek */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 300;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBxc4EsA.woff2) format('woff2');
        unicode-range: U+0370-03FF;
    }

    /* latin-ext */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 300;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fChc4EsA.woff2) format('woff2');
        unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 300;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format('woff2');
        unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }

    /* greek-ext */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7mxKOzY.woff2) format('woff2');
        unicode-range: U+1F00-1FFF;
    }

    /* greek */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4WxKOzY.woff2) format('woff2');
        unicode-range: U+0370-03FF;
    }

    /* latin-ext */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7GxKOzY.woff2) format('woff2');
        unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
        font-family: 'Roboto';
        font-style: normal;
        font-weight: 400;
        font-display: swap;
        src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4mxK.woff2) format('woff2');
        unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
</style>

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<body>
    <div class="container">
        <div class="row mb-2 mt-4" id="paper-title">
            <h1 class="col-md-12 text-center">
                Plenoxels
            </h1>
            <h3 class="col-md-12 text-center">
                Radiance Fields without Neural Networks
            </h3>
            <!-- <h3 class="col-md-12 text-center">
                <small>arXiv</small>
            </h3> -->
        </div>

        <div class="row" id="authors">
            <div class="mx-auto text-center">
                <ul class="list-inline mb-0">
                    <li class="list-inline-item">
                        <a href="https://alexyu.net">Alex Yu</a>
                        <sup>*</sup>
                    </li>

                    <li class="list-inline-item">
                        <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>
                        <sup>*</sup>
                    </li>

                    <li class="list-inline-item">
                        <a href="https://www.matthewtancik.com">Matthew Tancik</a>
                    </li>

                    <li class="list-inline-item">
                        <a href="https://www.linkedin.com/in/qinhong-chen/">Qinhong Chen</a>
                    </li>

                    <li class="list-inline-item">
                        <a href="http://people.eecs.berkeley.edu/~brecht/">Benjamin Recht</a>
                    </li>

                    <li class="list-inline-item">
                        <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>
                    </li>
                </ul>
                <ul class="list-inline mb-0" id="institution">
                    <li class="list-inline-item">
                        UC Berkeley
                    </li>
                    <li class="list-inline-item">
                        <sup>*</sup>
                        Equal contribution
                    </li>
                </ul>
            </div>
        </div>
        <div class="row mb-2" id="links">
            <div class="mx-auto">
                <ul class="nav">
                    <li class="nav-item text-center">
                        <a href="https://arxiv.org/abs/2112.05131" class="nav-link" title="Temp link">
                            <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                            </svg><br>
                            Paper
                        </a>
                    <li class="nav-item text-center">
                        <a href="https://github.com/sxyu/svox2" class="nav-link">
                            <svg style="width:48px;height:48px" viewBox="0 0 65 65">
                                <path fill="currentColor" d="M32 0a32.021 32.021 0 0 0-10.1 62.4c1.6.3 2.2-.7 2.2-1.5v-6c-8.9 1.9-10.8-3.8-10.8-3.8-1.5-3.7-3.6-4.7-3.6-4.7-2.9-2 .2-1.9.2-1.9 3.2.2 4.9 3.3 4.9 3.3 2.9 4.9 7.5 3.5 9.3 2.7a6.93 6.93 0 0 1 2-4.3c-7.1-.8-14.6-3.6-14.6-15.8a12.27 12.27 0 0 1 3.3-8.6 11.965 11.965 0 0 1 .3-8.5s2.7-.9 8.8 3.3a30.873 30.873 0 0 1 8-1.1 30.292 30.292 0 0 1 8 1.1c6.1-4.1 8.8-3.3 8.8-3.3a11.965 11.965 0 0 1 .3 8.5 12.1 12.1 0 0 1 3.3 8.6c0 12.3-7.5 15-14.6 15.8a7.746 7.746 0 0 1 2.2 5.9v8.8c0 .9.6 1.8 2.2 1.5A32.021 32.021 0 0 0 32 0z" />
                            </svg>
                            <br>
                            Code
                        </a>
                </ul>
            </div>
        </div>
        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
                <div class="embed-responsive embed-responsive-16by9 pb-3">
                    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/KCDd7UFO1d0" frameborder="0" allow="accelerometer; autoplay muted; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
                <p class="text-justify mt-2 pt-3">
                    We propose a view-dependent sparse voxel model, Plenoxel <em>(plenoptic volume element)</em>, that can optimize to the same fidelity as <a href="http://tancik.com/nerf">Neural Radiance Fields (NeRFs)</a>
                    without any neural networks. Our typical optimization time is 11 minutes on a single GPU, a speedup of two orders of magnitude compared to NeRF.
                </p>
            </div>
        </div>
        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
                <img class="img-responsive" src="img/pipeline.png" alt="Pipeline (figure 2)">
                <p class="text-justify">
                    Given a set of calibrated images of an object or scene, we reconstruct a (a) sparse voxel (“Plenoxel”) grid with density and spherical harmonic coefficients at each voxel. To render a ray, we (b) compute the color and opacity of each sample point via trilinear interpolation of the neighboring voxel coefficients. We integrate the color and opacity of these samples using (c) differentiable volume rendering, following the recent success of NeRF. The voxel coefficients can then be (d) optimized using the standard MSE reconstruction loss relative to the training images, along with a total variation regularizer.
                </p>

            </div>
        </div>
        <div class="row mb-4">
            <div class="col-md-8 mx-auto">
                <h4 class="mb-3">BibTeX</h4>
                <textarea id="bibtex" class="form-control" readonly>
@misc{yu2021plenoxels,
      title={Plenoxels: Radiance Fields without Neural Networks},
      author={{Alex Yu and Sara Fridovich-Keil} and Matthew Tancik and Qinhong Chen and Benjamin Recht and Angjoo Kanazawa},
      year={2021},
      eprint={2112.05131},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}</textarea>
                <p>Note: joint first-authorship is not really supported in BibTex; you may need to modify the above if not using CVPR's format.</p>
            </div>
        </div>
        <div class="row mb-3 pt-1">
            <div class="col-md-8 mx-auto">
                <h4>Superfast Convergence</h4>
                <p>
                    Our method converges rapidly. We reach comparable metrics (PSNR) 100x faster than NeRF, and achieve reasonable results within a few seconds of optimization.
                </p>
                <video class="fullwid-vid" controls muted>
                    <source src="https://angjookanazawa.com/plenoxel_data/fastopt.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <div class="row mb-3 pt-1">
            <div class="col-md-8 mx-auto">
                <h4>Results on Forward-facing Scenes</h4>
                <p>
                    Using NeRF's NDC parameterization, we are able to approximately match NeRF's results on forward-facing scenes as well.
                </p>
                <video class="fullwid-vid" controls autoplay muted loop>
                    <source src="https://angjookanazawa.com/plenoxel_data/eight.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        <div class="row mb-3 pt-1">
            <div class="col-md-8 mx-auto">
                <h4>Results on 360&deg; Scenes</h4>
                <p>
                    We extend the method to 360&deg; real scenes using a background model based on multi-sphere images (MSI), similar to the approach used in <a href="https://arxiv.org/abs/2010.07492">NeRF++</a>.
                </p>
                <div class="row">
                    <div class="col-md-6 col-tight">
                        <video src="https://angjookanazawa.com/plenoxel_data/m60_close.mp4" class="fullwid-vid" controls autoplay muted loop>
                        </video>
                    </div>
                    <div class="col-md-6 col-tight">
                        <video src="https://angjookanazawa.com/plenoxel_data/playground.mp4" class="fullwid-vid" controls autoplay muted loop>
                        </video>
                    </div>
                </div>
            </div>
        </div>
        <div class="row mb-3 pt-1">
            <div class="col-md-8 mx-auto">
                <h4>Background / Foreground</h4>
                <p>
                    The following is a capture of a real Lego bulldozer. We show the background and foreground models separately.
                </p>
                <div class="row">
                    <div class="col-md-4 col-tight">
                        <video src="https://angjookanazawa.com/plenoxel_data/spiral2.mp4" class="fullwid-vid" controls autoplay muted loop>
                        </video>
                        <p class="fullwid-centered">
                            Full
                        </p>
                    </div>
                    <div class="col-md-4 col-tight">
                        <video src="https://angjookanazawa.com/plenoxel_data/spiral2_fg.mp4" class="fullwid-vid" controls autoplay muted loop>
                        </video>
                        <p class="fullwid-centered">
                            Foreground
                        </p>
                    </div>
                    <div class="col-md-4 col-tight">
                        <video src="https://angjookanazawa.com/plenoxel_data/spiral2_bg.mp4" class="fullwid-vid" controls autoplay muted loop>
                        </video>
                        <p class="fullwid-centered">
                            Background
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <div class="row mb-3">
            <div class="col-md-8 mx-auto">
                <h4>Concurrent Works</h4>
                <p>
                    Please also check out <a href="https://arxiv.org/abs/2111.11215">DirectVoxGo</a>, a similar work which recently appeared on arXiv. They use a neural net to fit the color, but do not make use of TV regularization, SH, or sparse voxels. Additionally, their implementation does not require custom CUDA kernels.
                </p>
            </div>
        </div>
        <div class="row mb-3">
            <div class="col-md-8 mx-auto">
                <h4>Acknowledgements</h4>
                <p class="text-justify">
                    We note that Utkarsh Singhal and Sara Fridovich-Keil tried a related idea with point clouds some time prior to this project. Additionally, we would like to thank Ren Ng for helpful suggestions and Hang Gao for reviewing a draft of the paper. The NeRF++ results were generously provided by the authors.
                </p>
                <p class="text-justify">
                    The project is generously supported in part by the CONIX Research Center,
                    one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA; a Google research award to Angjoo Kanazawa; Benjamin Recht’s ONR awards N00014-20-1-2497 and N00014-18-1-2833, NSF CPS award 1931853, and the DARPA Assured Autonomy program (FA8750-18-C-0101).
                    Sara Fridovich-Keil and Matthew Tancik are supported by the NSF GRFP.
                </p>
                <p class="text-justify">
                    This website is in part based on a template of <a href="http://mgharbi.com/">Micha&euml;l Gharbi</a>, also used in <a href="https://alexyu.net/pixelnerf">PixelNeRF</a> and <a href="https://alexyu.net/plenoctrees">PlenOctrees</a>.
                </p>
            </div>
        </div>
    </div> <!-- container -->
    <script>
        window.mobileAndTabletCheck = function() {
            let check = false;
            (function(a) {
                if (/(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino|android|ipad|playbook|silk/i.test(a) || /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i.test(a.substr(0, 4))) check = true;
            })(navigator.userAgent || navigator.vendor || window.opera);
            return check;
        };

        if (window.location.host.indexOf('alexyu.net') > -1 && window.location.protocol != "https:") {
            // Force HTTPS
            window.location.protocol = "https";
        }


        if (mobileAndTabletCheck()) {
            document.getElementById('demo-warning').style.display = 'block';
            document.getElementById('demo-container').style.display = 'none';
            document.getElementById('demo-warning').innerHTML = "Unfortunately, mobile and tablet devices are not currently supported due to WebGL compatibility issues. We hope to support this in the future.";
        } else {
            var canvas = document.createElement('canvas');
            var gl = canvas.getContext('webgl');
            var tex_limit = gl.getParameter(gl.MAX_TEXTURE_SIZE);
            if (gl && gl instanceof WebGLRenderingContext) {
                const REQUIRED_TEX_LIMIT = 8192;
                if (tex_limit < REQUIRED_TEX_LIMIT) {
                    document.getElementById('demo-warning').style.display = 'block';
                    document.getElementById('demo-container').style.display = 'none';
                    document.getElementById('demo-warning').innerHTML = "Your GPU's maximum texture size is: " + tex_limit + " which is less than the minimum required (" + REQUIRED_TEX_LIMIT + ").  Please try another device, if possible.";
                }
            } else {
                document.getElementById('demo-warning').style.display = 'block';
                document.getElementById('demo-container').style.display = 'none';
                document.getElementById('demo-warning').innerHTML = "Your browser does not support WebGL, or WebGL was disabled. Please use a modern browser like Chrome or Firefox.";
            }
        }
    </script>
</body>
